<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Supply Chain on Mrugank M Akarte</title>
    <link>http://localhost:1313/categories/supply-chain/</link>
    <description>Recent content in Supply Chain on Mrugank M Akarte</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2019 13:00:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/supply-chain/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inventory Optimization: MDP vs RL</title>
      <link>http://localhost:1313/blog/inventory-optimization-with-reinforcement-learning/</link>
      <pubDate>Tue, 04 Jun 2019 13:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/inventory-optimization-with-reinforcement-learning/</guid>
      <description>&lt;p&gt;Inventory Optimization is a task of maximizing revenue by taking into account the capital investment, warehouse capacity, supply and demand of stock, leadtime and backordering of stocks. This problem has been well researched and is usually presented in form of a Markov Decision Process (MDP). The (s, S) policy is proved to be a optimal solution for such problems.[s: Reorder stock level, S: Target stock level].&lt;/p&gt;&#xA;&lt;p&gt;Markov Decision Process (MDP) provide a framework to model decision making process where outcomes are partly random and partly under the control of decision maker. The learner or decision maker is called an &lt;em&gt;agent&lt;/em&gt;. The agent interacts with the &lt;em&gt;environment&lt;/em&gt; which comprises of everything except the agent.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
