<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning on Mrugank M Akarte</title>
    <link>http://localhost:1313/categories/reinforcement-learning/</link>
    <description>Recent content in Reinforcement Learning on Mrugank M Akarte</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2019 13:00:00 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Inventory Optimization: MDP vs RL</title>
      <link>http://localhost:1313/blog/inventory-optimization-with-reinforcement-learning/</link>
      <pubDate>Tue, 04 Jun 2019 13:00:00 +0530</pubDate>
      <guid>http://localhost:1313/blog/inventory-optimization-with-reinforcement-learning/</guid>
      <description>&lt;p&gt;Inventory Optimization is a task of maximizing revenue by taking into account the capital investment, warehouse capacity, supply and demand of stock, leadtime and backordering of stocks. This problem has been well researched and is usually presented in form of a Markov Decision Process (MDP). The (s, S) policy is proved to be a optimal solution for such problems.[s: Reorder stock level, S: Target stock level].&lt;/p&gt;&#xA;&lt;p&gt;Markov Decision Process (MDP) provide a framework to model decision making process where outcomes are partly random and partly under the control of decision maker. The learner or decision maker is called an &lt;em&gt;agent&lt;/em&gt;. The agent interacts with the &lt;em&gt;environment&lt;/em&gt; which comprises of everything except the agent.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep Q-Learning</title>
      <link>http://localhost:1313/blog/deep-q-learning/</link>
      <pubDate>Sat, 18 May 2019 10:00:31 +0530</pubDate>
      <guid>http://localhost:1313/blog/deep-q-learning/</guid>
      <description>&lt;h1 id=&#34;deep-q-learning&#34;&gt;Deep Q-learning&lt;/h1&gt;&#xA;&lt;p&gt;We introduce deep neural networks to do the Q-Learning, hence the name Deep Q-Learning. Instead of calculating Q-values for each state-action pair, we calculate Q-values for all actions given the state and then select the action with maximum q-value. This concept was first introduced in &lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34;&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt; paper. The authors show that they were able to surpass human experts on three out of seven Atari games tested using deep neural networks to solve these reinforcement problems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Q-Learning</title>
      <link>http://localhost:1313/blog/q-learning/</link>
      <pubDate>Sun, 12 May 2019 20:00:31 +0530</pubDate>
      <guid>http://localhost:1313/blog/q-learning/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://localhost:1313/blog/ql/q-learning.webp&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;q-learning&#34;&gt;Q-Learning&lt;/h2&gt;&#xA;&lt;p&gt;Q-Learning is a value based reinforcement algorithm. The idea is that we create a &lt;em&gt;Q-Table&lt;/em&gt; which has all the &lt;em&gt;states&lt;/em&gt; represented as &lt;em&gt;rows&lt;/em&gt; of Q-table and &lt;em&gt;actions&lt;/em&gt; as &lt;em&gt;columns&lt;/em&gt;. Then for each state we would select an action which has maximum value (q-value). This means that we do not change/implement a policy that our agent will follow, instead we improve our Q-Table to always choose the best possible action. Lets take an example of Frozen Lake game. The environment is as shown in following image.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Reinforcement Learning</title>
      <link>http://localhost:1313/blog/reinforcement-learning/</link>
      <pubDate>Mon, 06 May 2019 20:00:31 +0530</pubDate>
      <guid>http://localhost:1313/blog/reinforcement-learning/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Reinforcement learning is a type of machine learning problem which is solved using the past experiences and interactions of the agent with the world. In other words, Reinforcement learning is mappings of situations with the available actions to maximize a numerical reward signal. Usually rewards are positive if the action taken is desirable and negative if the action is undesirable. The agent is never told which action to take or which action is optimal for a given situation, but instead it must discover on its own which actions to take that would yield maximum reward. In many cases the action taken for a given situation may affect not only immediate reward but also the next situation and may have consequences on future rewards. These characteristics of trial and error search and delayed rewards are distinguishing features of reinforcement learning.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
