<!DOCTYPE html>
<html class="no-js" lang="en-us" scroll-behavior="smooth"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Reinforcement Learning Application">
    <meta name="author" content="Mrugank Akarte">

    <!-- Mobile Specific Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.55.4" />

    <title>Inventory Optimization: MDP vs RL | Blogs</title>


    <!-- Favicon -->
    <link rel="shortcut icon" type="image/x-icon" href="https://mrugankakarte.github.io/images/favicon.ico"/>

    <!-- CSS -->
    <!-- Fontawesome Icon font -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/themefisher-font/style.css">
    <!-- bootstrap.min css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/bootstrap/dist/css/bootstrap.min.css">
    <!-- Animate.css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/animate-css/animate.css">
    <!-- Magnific popup css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/magnific-popup/dist/magnific-popup.css">
    <!-- Slick Carousel -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick.css">
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick-theme.css">
    <!-- Main Stylesheet -->
    
    <link rel="stylesheet" href="https://mrugankakarte.github.io/css/style.min.css" integrity="" media="screen">
    <!-- Custom CSS -->
    
    <link rel="stylesheet" href="https://mrugankakarte.github.io/css/custom.css">
    

</head>
<body id="body" data-spy="scroll" data-target=".navbar" data-offset="52">
        <div id="content">

<!-- Fixed Navigation -->
<nav id="navigation" class="navbar navbar-expand-lg navigation sticky-top">
        <div class="container">

                <!-- logo -->
                <a class="navbar-brand logo" href="https://mrugankakarte.github.io">
                        <img src="https://mrugankakarte.github.io/images/logo.png" alt="Logo" />
                        <span class="pl-2 font-weight-bold">Mrugank Milind Akarte</span>
                </a>
                <!-- /logo -->

                <!-- responsive nav button -->
                <button class="navbar-toggler navbar-dark" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                </button>
                <!-- /responsive nav button -->

                <!-- main nav -->
                
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ml-auto navigation-menu">
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#body">Home</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#about">About Me</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#blog">Blog</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#contact-us">Contact</a></li>
                                
                        </ul>
                </div>
                
                <!-- /main nav -->
        </div>

</nav>
<!-- End Fixed Navigation -->


<section class="section">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 offset-lg-2 text-center">
                <h1>Inventory Optimization: MDP vs RL</h1>
                <ul class="list-inline mb-50">
                    <li class="list-inline-item"><a href="/author/mrugank-akarte">Mrugank Akarte</a></li>
                    <li class="list-inline-item">Tuesday, Jun 4, 2019</li>
                </ul>
                <img class="img-fluid mb-50" src="https://mrugankakarte.github.io/images/blog/inventory_optimization.webp" alt="blog-image">
            </div>
            <div class="col-lg-8 offset-lg-2">
                <div class="post-single-content">
                    

<p>Inventory Optimization is a task of maximizing revenue by taking into account the capital investment, warehouse capacity, supply and demand of stock, leadtime and backordering of stocks. This problem has been well researched and is usually presented in form of a Markov Decision Process (MDP). The (s, S) policy is proved to be a optimal solution for such problems.[s: Reorder stock level, S: Target stock level].</p>

<p>Markov Decision Process (MDP) provide a framework to model decision making process where outcomes are partly random and partly under the control of decision maker. The learner or decision maker is called an <em>agent</em>. The agent interacts with the <em>environment</em> which comprises of everything except the agent.</p>

<p>The process flow in a MDP is as follows:<br />
1. We get the current state from the environment<br />
2. Agent takes action from all possible actions for the given state<br />
3. Environment reacts to the action taken by agent and present the reward and next state<br />
4. We modify the agents policy based on the rewards received<br />
5. Agent again acts for the new state using the modified policy</p>

<p>This process continues until the task is accomplished.</p>

<h2 id="inventory-optimization-with-mdp">Inventory Optimization with MDP</h2>

<p>Let&rsquo;s try to understand how to model inventory optimization problem into MDP.</p>

<p><img src="/blog/images/invopt-ex1.PNG" alt="invopt-ex1" title="Inventory Problem" />(Source: Andrew Schaefer, EWO Seminar PPT, Oct 2006)</p>

<p>In terms of MDP,</p>

<ul>
<li><p>The <em>states</em> are amount of stock available on hand at start of each time period. Let&rsquo;s assume the max inventory of our warehouse is M, then all the possible state values can be <em>S<sub>t</sub> = 0,1,2,3&hellip;.M</em></p></li>

<li><p>The <em>actions</em> are amount of stock that can be ordered at each time period. The possible values for action can be <em>a<sub>t</sub> = 0,1,2,3&hellip;..M-s<sub>t</sub></em></p></li>

<li><p>The <em>rewards</em> are expected net income for taking an action in a state. For inventory problem we have 3 costs associated to it as shown above.</p>

<ul>
<li>Ordering cost</li>
<li>Holding cost</li>
<li>Revenue generated after meeting the customer demand
<br /></li>
</ul>

<p>Thus, <strong>total reward = Expected revenue - order cost - holding cost</strong></p></li>

<li><p><em>Transition probabilites</em> govern how the state changes as the actions are taken over time. <img src="/blog/images/invopt-transprob.PNG" alt="invopt-transprob" title="Transition probabilites for inventory problem" /></p></li>
</ul>

<p>(<em>This transition probabilities are for a case where backorders are not accounted for</em>)</p>

<p>The obejective is <em>How much stock should be ordered at each time period such that my expected profits are maximized over time</em>. To solve this problem we make few assumptions:</p>

<ol>
<li>Maximum allowable inventory is M and units can be backordered if sufficient stock is not available to met the customer demand.</li>
<li>Demand is random with uniform probability distribution and the values can be 1,2,3,4&hellip;.M</li>
<li>The inventory (state) is counted at the beginning of each time period and a decision is then made to raise the stocks to M.</li>
<li>The ordered stock does not have any leadtime and are delivered instanteneously.</li>
<li>Customer demand is met at the end of each time period.</li>
</ol>

<p>With above assumptions, we use (s, S) policy to determine the amount of stock to reorder. In other words, we the stock falls below <em>s</em>, we order the amount such that the inventory is filled up to <em>S</em>.</p>

<p>Let&rsquo;s take an example:</p>

<p>We have maximum inventory size of 100 units, reorder level (s) is 40 units, restock level (S) = 100, it is (40, 100) policy. We haven&rsquo;t found the best policy here, which can be found out using value iteration and policy iteration methods. Order cost is 2 per unit, holding cost is 1 per unit, penalty for backorder is 2 per unit, revenue generated is 8 per unit. Check this <a href="/files/invopt-mdp.html">notebook</a> for complete code.</p>

<p>The average cost per episode with 100 timesteps is 180. The following diagram shows the state vs timestep. Notice, the state value doesn&rsquo;t go back to 100 in next state as mentioned in our policy, this is because the demand is met at the end of the time period which is subtracted.
<img src="/blog/images/invopt-mdp.png" alt="invopt-mpd" /></p>

<p>Let&rsquo;s use this as a baseline to test our model trained using Reinforcement Learning for Inventory Optimization.</p>

<h2 id="inventory-optimization-with-reinforcement-learning">Inventory Optimization with Reinforcement Learning</h2>

<p>The limitations of MDP is that as the problem size increases, i.e. increase in state and action size, it becomes coputationally difficult to solve the MDPs. Also for each state and action pair we need transition probability which may not be available in real world problem.</p>

<p>Using reinforcement learning removes the need of transition probabilities, and function approximators like neural networks can be used to model enormous state and action space. It is also easier to implement than MDPs.</p>

<p>We create two model types <strong>&lsquo;withdemand&rsquo;</strong> and <strong>&lsquo;nodemand&rsquo;</strong>. In model type <em>withdemand</em>, the agent is provided with the values of current demand along with current <em>state values</em> to predict the action. Whereas in model type <em>nodemand</em>, the current demand is not disclosed to the agent and the predictions are made based on only current <em>state values</em>. For both model type other information remains the same as defined in MDP. Please check this <a href="/files/invopt-rl1.html">notebook</a> for full code.</p>

<p>Following plot shows Inventory level vs Timestep for <em>nodemand</em> model type. After training the agent for 500 episodes with 100 timestes each, the average cost per episode with 100 timesteps is 180. From the plot we can see that agent is struggling to keep the backorders and state value to minimum as the demand is unknown.</p>

<p><img src="/blog/images/invopt-nodemand.png" alt="nodemand" title="Model:nodemand" /></p>

<p>Following graph shows Inventory level vs Timestep for <em>withdemand</em> model type. After training the agent for 500 episodes with 100 timestes each, the average cost per episode with 100 timesteps is 280. The agent is trying to keep the <em>state</em> value close to zero to keep the holding cost to minimum and hence maximizing the rewards.</p>

<p><img src="/blog/images/invopt-withdemand.png" alt="withdemand" title="Model:withdemand" /></p>

<p>These models were sucessful in increasing the average profits over a period of time compared to MDP.</p>

<h3 id="introducing-lead-time-for-delivery-of-stock">Introducing lead time for delivery of stock</h3>

<p>Let&rsquo;s up the game by adding leadtime to the above problem. Assume that there is a lead time of 3 time periods, i.e the action taken at time period <em>t</em> will reach the warehouse at time period <em>t+3</em>. The demand is random with uniform distribution but, it limited to maximum quantity of 30 unit at each time period. In such cases the maximum value of inventory can exceed the value M, thus they are usually modeled as infinite warehouse capacity, i.e there is no upper limit to stocks that can be stored in a warehouse. Although it doesn&rsquo;t sound correct, but when we train our agent, the agent realizes that keeping excess stocks reduces his profits and thus will try to keep the stock on hand to minimum.</p>

<p>Following graph shows <strong><em>Current state vs Timeperiod</em></strong> for an agent trained for 300 episodes with 500 timesteps each. The current state and demand are given as input to agent to predict the action. The average profit per episode is 30. This is very low as compared to previous solutions, this is because of two main reasons. One, the demand is random and second the agent doesn&rsquo;t know that there is a lead time of 3 units for the actions taken. Hence, it becomes very difficult to predict the actions so that holding costs are minimized to maximize the profits. The <strong>red line</strong> indicates the stocks on hand and <strong>orange line</strong> indicates the action taken (stock ordered) at time period <em>t</em>.
<img src="/blog/images/invopt-rl_leadtime3.png" alt="invopt-rl_leadtime" /></p>

<p>The following graph if for the same agent trained for 400 episodes with 500 timesteps each. It looks like the agent here is trying to keep the order cost to minimum and is therefore ordering large quantities of stock, thus ends up with high on-hand inventory.</p>

<p><img src="/blog/images/invopt-rl_leadtime2.png" alt="invopt-rl_leadtime" /></p>

<p>The following graph if for the same agent trained for 500 episodes with 500 timesteps each. It is visible that agent has realized it is keep a large stock on hand and thus tries to reduce it. The maximum stock on-hand dropped from 250 to 150, which show that agent partially successful in reducing the on-hand inventory. But now many of the orders are getting backlogged.</p>

<p><img src="/blog/images/invopt-rl_leadtime.png" alt="invopt-rl_leadtime" /></p>

<p>There are lots of areas to improve the results further, like giving the <em>stocks in transit</em> as input to our agent along with the current <em>state</em> and <em>demand</em>, testing different hyperparameters for neural net used in our agent, training the agent for longer duration. Complete code can be found <a href="/files/invopt-rl_leadtime.html">here</a>
This is just a small example of how reinforcement learning can be used to optimize inventory and maximize the profits in real world.</p>

                    
                        

<div class="social-share pt-4">
        <h4>Share:</h4>
        
        <a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg>
            </div>
        </div>
    </a>

    
    <a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?text=Inventory%20Optimization%3a%20MDP%20vs%20RL&amp;url=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_blank" rel="noopener" aria-label="">
        <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg>
        </div>
    </div>
</a>


<a class="resp-sharing-button__link" href="https://plus.google.com/share?url=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--google resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M11.37 12.93c-.73-.52-1.4-1.27-1.4-1.5 0-.43.03-.63.98-1.37 1.23-.97 1.9-2.23 1.9-3.57 0-1.22-.36-2.3-1-3.05h.5c.1 0 .2-.04.28-.1l1.36-.98c.16-.12.23-.34.17-.54-.07-.2-.25-.33-.46-.33H7.6c-.66 0-1.34.12-2 .35-2.23.76-3.78 2.66-3.78 4.6 0 2.76 2.13 4.85 5 4.9-.07.23-.1.45-.1.66 0 .43.1.83.33 1.22h-.08c-2.72 0-5.17 1.34-6.1 3.32-.25.52-.37 1.04-.37 1.56 0 .5.13.98.38 1.44.6 1.04 1.84 1.86 3.55 2.28.87.23 1.82.34 2.8.34.88 0 1.7-.1 2.5-.34 2.4-.7 3.97-2.48 3.97-4.54 0-1.97-.63-3.15-2.33-4.35zm-7.7 4.5c0-1.42 1.8-2.68 3.9-2.68h.05c.45 0 .9.07 1.3.2l.42.28c.96.66 1.6 1.1 1.77 1.8.05.16.07.33.07.5 0 1.8-1.33 2.7-3.96 2.7-1.98 0-3.54-1.23-3.54-2.8zM5.54 3.9c.33-.38.75-.58 1.23-.58h.05c1.35.05 2.64 1.55 2.88 3.35.14 1.02-.08 1.97-.6 2.55-.32.37-.74.56-1.23.56h-.03c-1.32-.04-2.63-1.6-2.87-3.4-.13-1 .08-1.92.58-2.5zM23.5 9.5h-3v-3h-2v3h-3v2h3v3h2v-3h3"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=Inventory%20Optimization%3a%20MDP%20vs%20RL&amp;body=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_self" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9 0 6v12c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17 0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1 0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08 0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f&amp;resubmit=true&amp;title=Inventory%20Optimization%3a%20MDP%20vs%20RL" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M24 11.5c0-1.65-1.35-3-3-3-.96 0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65 0 3-1.35 3-3s-1.35-3-3-3c-1.38 0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65 0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66 0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64 0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4 0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6 0 .23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1 0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1 0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=Inventory%20Optimization%3a%20MDP%20vs%20RL%20https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3 0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7 0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1 0-5.2 4.2-9.4 9.4-9.4 2.5 0 4.9 1 6.7 2.8 1.8 1.8 2.8 4.2 2.8 6.7-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2-.2-.3 0-.5.1-.6s.3-.3.4-.5c.2-.1.3-.3.4-.5.1-.2 0-.4 0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Inventory%20Optimization%3a%20MDP%20vs%20RL&amp;url=https%3a%2f%2fmrugankakarte.github.io%2fblog%2finventory-optimization-with-reinforcement-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M.707 8.475C.275 8.64 0 9.508 0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102 0 0 0 1.75.527l2.96-2.41a.405.405 0 0 1 .494-.013l5.34 3.87a1.1 1.1 0 0 0 1.046.135 1.1 1.1 0 0 0 .682-.803l3.91-18.795A1.102 1.102 0 0 0 22.5.075L.706 8.475z"/></svg>
    </div>
</div>
</a>

</div>
                    
                    

                </div>
            </div>
        </div>
    </div>
</section>


        </div><!-- end Contact Area -->
<footer id="footer" class="bg-one">
	<div class="container">
		<div class="row wow fadeInUp" data-wow-duration="500ms">
			<div class="col-xl-12">

				<!-- Footer Social Links -->
				<div class="social-icon">
					<ul class="list-inline">
						
						<li class="list-inline-item"><a href="https://www.linkedin.com/in/mrugank-akarte/"><i class="tf-ion-social-linkedin"></i></a></li>
						
						<li class="list-inline-item"><a href="https://www.instagram.com/mrugankakarte/"><i class="tf-ion-social-instagram"></i></a></li>
						
						<li class="list-inline-item"><a href="https://github.com/Mrugankakarte"><i class="tf-ion-social-github"></i></a></li>
						
					</ul>
				</div>

				<!-- copyright -->
				<div class="copyright text-center">
					<a href="https://mrugankakarte.github.io">
						<img src="https://mrugankakarte.github.io/images/logo.png" alt="Meghna" />
					</a>
					<br>
					<p>Design And Developed by <a href="http://www.themefisher.com">  Themefisher Team  </a>. Copyright &copy; <script>
							document.write(new Date().getFullYear())
						</script>. All Rights Reserved.</p>
				</div>
			</div>
		</div>
	</div>
</footer>
<!-- /footer -->

<!-- Essential Scripts -->

		<!-- Main jQuery -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/jquery/dist/jquery.min.js"></script>
		<!-- Bootstrap 4.3 + Popper -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
		<!-- Slick Carousel -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick.min.js"></script>
		<!-- Portfolio Filtering -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/filterzr/jquery.filterizr.min.js"></script>
		<!-- Magnific popup -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/magnific-popup/dist/jquery.magnific-popup.min.js"></script>
		<!-- Google Map API -->
		<script type="text/javascript"  src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCcABaamniA6OL5YvYSpB3pFMNrXwXnLwU&amp;libraries=places"></script>
		<!-- Number Counter Script -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/count-to/jquery.countTo.js"></script>
		<!-- wow.min Script -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/wow/dist/wow.min.js"></script>
		<!-- Scroll behavior polyfill -->
		
		<script src="https://mrugankakarte.github.io/js/scroll-behavior-polyfill.min.js"></script>
		<!-- Sweet Alert -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/sweet-alert/sweetalert.min.js"></script>
		<!-- Custom js -->
		
		<script src="https://mrugankakarte.github.io/js/script.min.js"></script>
		<!-- google analitycs -->
		<script>
				(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
				})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
				ga('create', 'UA-139328350-1', 'auto');
				ga('send', 'pageview');
		</script>

    </body>
</html></body>
</html>
