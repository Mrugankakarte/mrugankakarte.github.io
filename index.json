[{"categories":["Games"],"contents":"Project MineRL: Sample efficient reinforcement learning using human prior\nIntroduction A challenge to develop a system to obtain a diamond in Minecraft using limited amount of training time. Since the task is super hard, the organizers also created a smaller problems like chopping trees, navigate to a point, obtain an iron pickaxe. In this post I am going to share my experience about solving the navigate to a point problem.\nFor those who don\u0026rsquo;t know what Minecraft is, let me tell you about it briefly. Minecraft is a sandbox game with a 3D world in a block structure. Every object in the game is made from combination of sqaure blocks. I suggest you take a quick look at the game trailer on youtube here to get an idea what the game is about.\nSource: https://www.minecraft.net/en-us/\nThere are two modes Survival and Creative in which a player can play the game. For our challenge we will be using the survival mode, which means that the agent has limited health and taking damage can reasult in death. We will be using the \u0026lsquo;minerl\u0026rsquo; python package developed by the organizers to take care of the environmental dynamics. Thus our focus is to control and teach the agent to complete the tasks using reinforcement learning.\nFrom now on I will be discussing about the navigate to a point task. Let\u0026rsquo;s take a look at the inputs from the game and the actions available for the agent (available at minerl.io).\nObservation Space:\nDict({\r\u0026#34;compassAngle\u0026#34;: \u0026#34;Box()\u0026#34;,\r\u0026#34;inventory\u0026#34;: {\r\u0026#34;dirt\u0026#34;: \u0026#34;Box()\u0026#34;\r},\r\u0026#34;pov\u0026#34;: \u0026#34;Box(64, 64, 3)\u0026#34;\r}) The compassAngle variable has observation which points near the goal location. The agent must find the unique block (Blue Diamond block) at goal location to complete the task. The inventory variable stores amount of dirt blocks collected, which can be used to climb to locations that cannot be accessed by jumping. The pov variable has a frame (RGB image) from the game with size 64x64x3.\nAction Space:\nDict({\r\u0026#34;attack\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;back\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;camera\u0026#34;: \u0026#34;Box(2,)\u0026#34;,\r\u0026#34;forward\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;jump\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;left\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;place\u0026#34;: \u0026#34;Enum(none,dirt)\u0026#34;,\r\u0026#34;right\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;sneak\u0026#34;: \u0026#34;Discrete(2)\u0026#34;,\r\u0026#34;sprint\u0026#34;: \u0026#34;Discrete(2)\u0026#34;\r}) The action space consists of movements (forward, back, left, right, jump, sneak, sprint) and other actions like attack, place. The camera variable controls the vision of the agent in horizontal and vertical direction. Discrete(2) means that the variable can take only 2 values, either 0 or 1. Box(2,) means that the variable contains two real values like [0.12, -0.36].\nLet\u0026rsquo;s take a look at how the reward function is defined for this problem. In \u0026lsquo;minerl\u0026rsquo; there are two options to train the model NavigateDense and Navigate. In NavigateDense variant, the agent receives reward every tick for how close or far it is from the objective and for the Navigate variant the agent only receives reward when it reaches the objective. I used the NavigateDense variant to train the model as it would help the agent learn faster.\nUsing human priors for pre-training model Disclaimer: The output video is 64*64 by default hence the video looks blurry.\nHuman priors in this case are recording of various players who performed the same task. Along with video the actions were also recorded. One example is as shown below.\n(Source: minerl.viewer from minerl package)\nThe red marks on arrows and other button indicates the action taken at that time period, the camera control records the movement of agents vision and on top of camera control we can see the reward as function of time. We observe reward obtain at each tick along with cumulative reward. There are 194 such trajectories for NavigateDense variant and 191 trajectories for Navigate variant.\nMy approach to solve the Navigation task Since the competition is still in progress, I will only be showing the video clips of my agent trained so far and a brief overview of how I achieved it.\nAfter doing some research on available Reinforcement learning algorithms, I found the DDPG algorithm to be most suitable for this task. This algorithm was developed to solve the problem where there are continuous action spaces. I used the similar model architecture as mentioned in DDPG paper with some modifications to account of our actions. I tried the imitation learning algorithms to pre-train my models on human trajectories. I used Google Colab to pre-train the models. I am still training my agent to get better result and will post the final version of the agent later.\nFollowing video shows agent taking random actions without pre-training from human priors.\nWe can see that it is very difficult for agent to reach the objective if we continue this process (we might even never reach the goal!). To make this task easier for the agent we try imitation learning (showing the agent how the task is done), basically we try to copy the human actions as it is. Following video shows the agent behavior after pre-training on human priors after few iterations.\nAwesome!! That\u0026rsquo;s a great improvement, don\u0026rsquo;t you think? The agent still doesn\u0026rsquo;t know it is supposed to touch the special block (Blue Diamond block) to complete it\u0026rsquo;s task. Also it starts spinning in circle when it is closer to the target. This point is better to learn from self memory as agent now knows he is supposed to follow the compass. After training from self memory, I believe that the agent will be able to complete the navigation task. I will update this post once I get it running.\nStay tuned for more videos and posts!!!\n","permalink":"http://localhost:1313/blog/project-minerl/","tags":["Minecraft"],"title":"Project MineRL"},{"categories":["Reinforcement Learning","Supply Chain"],"contents":"Inventory Optimization is a task of maximizing revenue by taking into account the capital investment, warehouse capacity, supply and demand of stock, leadtime and backordering of stocks. This problem has been well researched and is usually presented in form of a Markov Decision Process (MDP). The (s, S) policy is proved to be a optimal solution for such problems.[s: Reorder stock level, S: Target stock level].\nMarkov Decision Process (MDP) provide a framework to model decision making process where outcomes are partly random and partly under the control of decision maker. The learner or decision maker is called an agent. The agent interacts with the environment which comprises of everything except the agent.\nThe process flow in a MDP is as follows:\nWe get the current state from the environment Agent takes action from all possible actions for the given state Environment reacts to the action taken by agent and present the reward and next state We modify the agents policy based on the rewards received Agent again acts for the new state using the modified policy This process continues until the task is accomplished.\nInventory Optimization with MDP Let\u0026rsquo;s try to understand how to model inventory optimization problem into MDP.\n(Source: Andrew Schaefer, EWO Seminar PPT, Oct 2006)\nIn terms of MDP,\nThe states are amount of stock available on hand at start of each time period. Let\u0026rsquo;s assume the max inventory of our warehouse is M, then all the possible state values can be St = 0,1,2,3\u0026hellip;.M\nThe actions are amount of stock that can be ordered at each time period. The possible values for action can be at = 0,1,2,3\u0026hellip;..M-st\nThe rewards are expected net income for taking an action in a state. For inventory problem we have 3 costs associated to it as shown above. - Ordering cost - Holding cost - Revenue generated after meeting the customer demand Thus,\ntotal reward = Expected revenue - order cost - holding cost\rTransition probabilites govern how the state changes as the actions are taken over time. (This transition probabilities are for a case where backorders are not accounted for)\nThe obejective is How much stock should be ordered at each time period such that my expected profits are maximized over time. To solve this problem we make few assumptions:\nMaximum allowable inventory is M and units can be backordered if sufficient stock is not available to met the customer demand. Demand is random with uniform probability distribution and the values can be 1,2,3,4\u0026hellip;.M The inventory (state) is counted at the beginning of each time period and a decision is then made to raise the stocks to M. The ordered stock does not have any leadtime and are delivered instanteneously. Customer demand is met at the end of each time period. With above assumptions, we use (s, S) policy to determine the amount of stock to reorder. In other words, we the stock falls below s, we order the amount such that the inventory is filled up to S.\nLet\u0026rsquo;s take an example:\nWe have maximum inventory size of 100 units, reorder level (s) is 40 units, restock level (S) = 100, it is (40, 100) policy. We haven\u0026rsquo;t found the best policy here, which can be found out using value iteration and policy iteration methods. Order cost is 2 per unit, holding cost is 1 per unit, penalty for backorder is 2 per unit, revenue generated is 8 per unit. Check this notebook for complete code.\nThe average cost per episode with 100 timesteps is 180. The following diagram shows the state vs timestep. Notice, the state value doesn\u0026rsquo;t go back to 100 in next state as mentioned in our policy, this is because the demand is met at the end of the time period which is subtracted. Let\u0026rsquo;s use this as a baseline to test our model trained using Reinforcement Learning for Inventory Optimization.\nInventory Optimization with Reinforcement Learning The limitations of MDP is that as the problem size increases, i.e. increase in state and action size, it becomes coputationally difficult to solve the MDPs. Also for each state and action pair we need transition probability which may not be available in real world problem.\nUsing reinforcement learning removes the need of transition probabilities, and function approximators like neural networks can be used to model enormous state and action space. It is also easier to implement than MDPs.\nWe create two model types \u0026lsquo;withdemand\u0026rsquo; and \u0026rsquo;nodemand\u0026rsquo;. In model type withdemand, the agent is provided with the values of current demand along with current state values to predict the action. Whereas in model type nodemand, the current demand is not disclosed to the agent and the predictions are made based on only current state values. For both model type other information remains the same as defined in MDP. Please check this notebook for full code.\nFollowing plot shows Inventory level vs Timestep for nodemand model type. After training the agent for 500 episodes with 100 timestes each, the average cost per episode with 100 timesteps is 180. From the plot we can see that agent is struggling to keep the backorders and state value to minimum as the demand is unknown.\nFollowing graph shows Inventory level vs Timestep for withdemand model type. After training the agent for 500 episodes with 100 timestes each, the average cost per episode with 100 timesteps is 280. The agent is trying to keep the state value close to zero to keep the holding cost to minimum and hence maximizing the rewards.\nThese models were sucessful in increasing the average profits over a period of time compared to MDP.\nIntroducing lead time for delivery of stock Let\u0026rsquo;s up the game by adding leadtime to the above problem. Assume that there is a lead time of 3 time periods, i.e the action taken at time period t will reach the warehouse at time period t+3. The demand is random with uniform distribution but, it limited to maximum quantity of 30 unit at each time period. In such cases the maximum value of inventory can exceed the value M, thus they are usually modeled as infinite warehouse capacity, i.e there is no upper limit to stocks that can be stored in a warehouse. Although it doesn\u0026rsquo;t sound correct, but when we train our agent, the agent realizes that keeping excess stocks reduces his profits and thus will try to keep the stock on hand to minimum.\nFollowing graph shows Current state vs Timeperiod for an agent trained for 300 episodes with 500 timesteps each. The current state and demand are given as input to agent to predict the action. The average profit per episode is 30. This is very low as compared to previous solutions, this is because of two main reasons. One, the demand is random and second the agent doesn\u0026rsquo;t know that there is a lead time of 3 units for the actions taken. Hence, it becomes very difficult to predict the actions so that holding costs are minimized to maximize the profits. The red line indicates the stocks on hand and orange line indicates the action taken (stock ordered) at time period t. The following graph if for the same agent trained for 400 episodes with 500 timesteps each. It looks like the agent here is trying to keep the order cost to minimum and is therefore ordering large quantities of stock, thus ends up with high on-hand inventory.\nThe following graph if for the same agent trained for 500 episodes with 500 timesteps each. It is visible that agent has realized it is keep a large stock on hand and thus tries to reduce it. The maximum stock on-hand dropped from 250 to 150, which show that agent partially successful in reducing the on-hand inventory. But now many of the orders are getting backlogged.\nThere are lots of areas to improve the results further, like giving the stocks in transit as input to our agent along with the current state and demand, testing different hyperparameters for neural net used in our agent, training the agent for longer duration. Complete code can be found here This is just a small example of how reinforcement learning can be used to optimize inventory and maximize the profits in real world.\n","permalink":"http://localhost:1313/blog/inventory-optimization-with-reinforcement-learning/","tags":["Supply Chain"],"title":"Inventory Optimization: MDP vs RL"},{"categories":["Reinforcement Learning"],"contents":"Deep Q-learning We introduce deep neural networks to do the Q-Learning, hence the name Deep Q-Learning. Instead of calculating Q-values for each state-action pair, we calculate Q-values for all actions given the state and then select the action with maximum q-value. This concept was first introduced in Playing Atari with Deep Reinforcement Learning paper. The authors show that they were able to surpass human experts on three out of seven Atari games tested using deep neural networks to solve these reinforcement problems.\nThe idea in the paper is that, you capture the Atari screen, use convolutional neural networks to extract the features of the game and then calculate the q-values for each action. Then action with maximum q-value is taken and reward, next state are observed. This data (current state, action taken, reward received, next state) are stored in a fixed sized buffer called replay memory. A batch of random samples with uniform distribution are drawn from this memory and then model is updated. The actions taken by our neural network model are considered as predictions and true actions are calculated using the bellman equation. The error between true value and predictions are calcluated as mean squared error and the model is optimized using this error.\n(Source: Playing Atari with Deep Reinfiorcement Learning, Dec 2013)\nLet\u0026rsquo;s try to understand what exactly is happening\u0026hellip;\nSimilar to Q-Learning, we get the state, execute the action using ε-greedy method and observe reward and next state. We then store these observations into a replay memory and use them to update the weights of our neural network which is used to take action. The neural network maps the state to the actions. In next step we sample the observations randomly from the replay memory and the reason we don\u0026rsquo;t take consecutive observations is that learning is inefficient due to high correlation between the samples. Randomizing the samples break this correlation which helps to reduce the variation of the updates for neural network.\nAnother reason is that current parameters determine the next data sample that the parameters are trained on. For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch. This can lead to high divergence in parameters and agent can get stuck in local minima. When randomized samples are used, this behaviour is averaged over many previous states thus smoothing out the learning.\nAnother confusing thing here is in calculation for values of ground truth. We are using the same network to calculate the yi and also for prediction. It\u0026rsquo;s like chasing your own tail. At everytime step we use our neural net to calculate the ground truth for the sampled states and also predict the Q-values for the same, update the network and repeat. So at each timestep we are changing our parameters which means the groud truth also changes. What this means is that everytime the model tries to reach the ground truth of a state, it changes to a new value. Apparently the model works.\nOne way to mitigate this problem is by using two neural nets (DQN network and Target network) say with parameters (θ and θ*). Initially we copy the parameters from DQN netwrok to Target network (θ -\u0026gt; θ*), but instead of updating the parameters of target network at each timestep we update them after some fixed time step. By updating the target parameter I mean we copy the new parameter values from DQN network to Targent network after fixed timesteps t. We then use the DQN network for predictions and Target network to calculate the ground truth.\nAnother solution to above problem is given in Deep Reinforcement learning using Double DQN. The idea is that when we compute the ground truth, we use two networks to decouple the selection from evaluation. So the equation in DQN\nchanges to\n(Notice the θ\u0026rsquo;).\nThis is done because it was found that when same network is used to select the best action and also for evaluation, the DQN tends to over-estimate values. Hence, we use the DQN network to select best action to take for next state (inner Q-value) argmax Q(St+1, a; θt) and the Target network to calculate Q(St+1, argmax Q(St+1, a; θt); θ\u0026rsquo;t) the ground truth value of taking that action at next state.\nLet\u0026rsquo;s see the DQN in action for Cart and Pole game\u0026hellip;\n1.Agent at the beginning of training\u0026hellip;(Random Actions)\n2.Agent after trained for 100 episodes\n3.Agent after trained for 200 episodes\nIsn\u0026rsquo;t it amazing!!! Just a few lines of code and our agent can balance a pole on a cart reasonably well. Infact this is not just limited to Cart and Pole Game, you can even train our agent to play some old school Atari games like SpaceInvaders, PingPong, Breakout and many more. But to get good results you need to train it for a long time. Here is the link for the complete code for running DQN for Cart and Pole.\nIn the next article, we will look at Policy Gradient Methods and solve the same Cart and Pole game to compare the results with Deep Q-Learning.\n","permalink":"http://localhost:1313/blog/deep-q-learning/","tags":["Reinforcement Learning"],"title":"Deep Q-Learning"},{"categories":["Reinforcement Learning"],"contents":"\nQ-Learning Q-Learning is a value based reinforcement algorithm. The idea is that we create a Q-Table which has all the states represented as rows of Q-table and actions as columns. Then for each state we would select an action which has maximum value (q-value). This means that we do not change/implement a policy that our agent will follow, instead we improve our Q-Table to always choose the best possible action. Lets take an example of Frozen Lake game. The environment is as shown in following image.\nS : Starting point, safe\nF : Frozen block, safe\nH : Hole, not safe\nG : Goal, Our destination\nThe objective is to determine the shortest possible path to the destination without falling in the hole. The episode ends when you reach the goal or fall in a hole. A reward of 1 is received if you reach the goal and 0 otherwise. In this case, our Q-Table has 16 states and 4 actions (left, right, up, down). Now the question is how do we calculate the q-values to fill the Q-Table? This is where Q-Learning comes into picture.\nGiven a state and action taken, the Q-function returns an expected discounted (γ = discount factor) future reward of that action for given state. This function is estimated using Q-learning, which updates Q(s,a) using bellman equation.\nQ-function Q(st, at) = E[Rt+1 + γRt+2 + γ2Rt+3 + \u0026hellip;|st, at]\nBellman Equation Q(St, At) = Q value for action taken a in state s at time t.\nalpha = learning rate of our algorithm.\nRt+1 = Reward received at time t+1 for action taken in previous state.\nQ-Learning Algorithm Let\u0026rsquo;s continue with the Frozen Lake game to understand this algorithm.\nInitialize the Q-table to random values. We initialize our Q-table which has 16 rows and 4 columns with zeros. Choose and take action. We chose action (a) for state (s) which has maximum q-value. But initially all the values are zero, this is where the exploration and exploitation strategy comes into play. We use a strategy called epsilon-greedy strategy.\nIn this we set the initial value of epsilon to 1 and gradually decrease the value to 0 with time. We draw a random number from a uniform distribution between 0 and 1. If this number is less than the epislon we take a random action. The idea behind this is that, initially as the agent doesn\u0026rsquo;t know anything about the environment, we take random action i.e we let the agent explore the environment. As agent explores the world, the epsilon value decreases and the agent starts exploiting. Eventually agent becomes confident in estimating the q-values.\nEvaluate the action taken. After taking action at each time step, we receive a reward. We then use this reward to update our Q-table using the bellman equation.\nRepeat. We repeat this process again and again untill the learning is stopped.\nCheck this Jupyter Notebook here for complete python code to solve the Frozen Lake problem using Q-Learning.\n(Source: Richard Sutton and Andrew Barto, Reinforcement learning Second Edition 2018)\nWe can see that when we have limited states, Q-table can be efficient. But what happens when we have lots and lots of states? For example you are playing \u0026lsquo;Ping-Pong\u0026rsquo;, there can be infinite states\u0026hellip;then how do we make Q-Table for such cases? The answer is Deep Q-Learning. In the next article we will learn more about Deep Q-Learning and solve a Cart \u0026amp; Pole problem.\n","permalink":"http://localhost:1313/blog/q-learning/","tags":[],"title":"Q-Learning"},{"categories":["Reinforcement Learning"],"contents":"Introduction Reinforcement learning is a type of machine learning problem which is solved using the past experiences and interactions of the agent with the world. In other words, Reinforcement learning is mappings of situations with the available actions to maximize a numerical reward signal. Usually rewards are positive if the action taken is desirable and negative if the action is undesirable. The agent is never told which action to take or which action is optimal for a given situation, but instead it must discover on its own which actions to take that would yield maximum reward. In many cases the action taken for a given situation may affect not only immediate reward but also the next situation and may have consequences on future rewards. These characteristics of trial and error search and delayed rewards are distinguishing features of reinforcement learning.\nReinforcement learning is different from supervised and unsupervised learning. Supervised learning is learning for a training set of known and labelled examples. Each example contains a set of features with its appropriate label. Reinforcement learning is an interactive problem in which it is often impossible to obtain the examples of desired behaviour that are both correct and representative of all situation for which the agent must act. Unsupervised learning is about finding hidden structure in an unlabelled dataset. Reinforcement learning is about maximizing the reward by taking actions in an unseen situation. Although finding hidden structure in data will be useful for learning but it does not address its the objective. Hence, Reinforcement is considered different from supervised and unsupervised machine learning.\nThe problem of reinforcement learning is formalized as a fully observed or partial Markov Decision Process (MDP). We will discuss MDP subsequently, first let’s discuss the basic terminologies used in reinforcement learning and one the most important challenges faced, trade-off between exploration and exploitation. To obtain a positive reward, the agent has to take right action for a given situation. Since the agent is never told what the optimal action is, the agent must take some action and experience the consequences. It must try all the actions available and then select the action which gives maximum reward. In this process the agent has to exploit what he already knows from the previous actions taken and explore all the possible options to make a better action selection in the future. The dilemma here is that neither exploration nor exploitation can be pursued exclusively without failing at task. The agent must try a variety of actions and progressively favour those which appear to be best.\n(Source: Richard Sutton and Andrew Barto, Reinforcement learning Second Edition 2018)\nReinforcement learning system Reinforcement learning system consists of a policy, reward signal, value function, and optionally a model of environment.\nPolicy defines what action to take at any given situation (hereafter referred as state). It is basically a mapping of actions to states of environment. It can be as simple as any function or a lookup table. Reward signal defines the goal of a reinforcement learning problem. After taking an action on a given state, the agent receives a reward which can be positive or negative. This reward is then used to adjust the policy in such a way that future actions are better. Positive rewards define what events are good for agent whereas negative rewards define the bad events. The sole objective of the agent is to maximize the reward over long term. Reward indicates what is good in immediate sense, Value function defines what is good in long run. It gives an estimate of total amount of reward that an agent can accumulate over future starting from that state. Model is something that can mimic an environment. For a given state and action, a model can predict the next state and next reward. Model can be used for planning and considering possible states before experiencing them. Reinforcement learning problems which use models are known as model-based methods. Model-free methods are purely based on trial-error learners. Markov Decision Process Markov Decision Process (MDP) provide a framework to model decision making process where outcomes are partly random and partly under the control of decision maker. The learner or decision maker is called an agent. The agent interacts with the environment which comprises of everything except the agent.\nThe process flow in a MDP is as follows:\nWe get the current state from the environment Agent takes action from all possible actions for the given state Environment reacts to the action taken by agent and present the reward and next state We modify the agents policy based on the rewards received Agent again acts for the new state using the modified policy This process continues until the task is accomplished.\nPolicy and Value Functions Value functions are functions that estimate \u0026ldquo;how good\u0026rdquo; it is for a agent to be in a given state. This is defined in terms of future rewards that can be expected. Value functions are defined with respect to particular ways of acting called policies.\nA policy function can be defined as mapping between the state and probability of selecting each possible action. Mathematically value function is defined as follows:\n(Source: Richard Sutton and Andrew Barto, Reinforcement learning Second Edition 2018)\nwhere, Eπ[.] denotes expected value of random variable given that agent follows the policy π and t is any time-step. This is also known as state-value function for policy π.\nSimilarly action-value function is defined as value of taking action a, given state s under policy π. (Source: Richard Sutton and Andrew Barto, Reinforcement learning Second Edition 2018)\nValue based reinforcement learning In value based learning, the objective is to optimize the value function. The agent will use this value function to select the action for a given state. The agent takes the action with the biggest value.\nPolicy based reinforcement learning In policy based reinforcement learning, the objective is to optimize the policy function π(s) The policy determines the probability for each possible action given a state and an action from this probability distribution is selected.\nIn next article we will discuss about the value based methods called Q-learning, Deep Q-Learning and its variants.\nPS: Most of the details are from Barto and Sutton, Reinforcement Book, Second Edition 2018. This post is my understanding of the topics from the book. If you would like to go in more details please refer the book.\n","permalink":"http://localhost:1313/blog/reinforcement-learning/","tags":["Reinforcement Learning"],"title":"Reinforcement Learning"},{"categories":["Text","NLP"],"contents":"This project is based on Kaggle Competition: Toxic Comment Classification Challenge\nThe challenge was to build a multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective’s current models. Comments from Wikipedia’s talk page edits were used as dataset to train models.\nThis was my first NLP Competition on Kaggle. Since everything was new to me, I learned a lot of new concepts, terms and techniques during the competiton.\nExploratory Data Analysis Disclaimer: Following content contains words that may be considered as vulger, profane or offensive.\nThe dataset was made up of 8 columns, id, Comments and 6 categories; toxic, severe_toxic, obscene, threat, insult and identity_hate. Dimensions of training set = 159571 x 8 and dimensions of Testing set = 153164 x 2\nMost frequent words in training set are shown in bar graph Following graph shows most common words in each category. Model Initially simple logistic regression was used to predict the probabilty of class being 1. Six such models were trained (one for each class). This model was used as baseline model. The input data to logistic regression was cleaned i.e numbers, english stopwords, puntuactions extra white spaces, Non-ASCII characters were removed, words were stemmed and converted to lower case.\nIn the era of Deep learning, I decided to train neural networks for this task. Special type of neural networks called Recurrent neural networks are used to solve problems which exhibit behaviour of time sequence. Keras, which is based on Tensorflow framework provides lot of options, from tokenizing sentences to training a RNN on GPU, to solve NLP related problem. Recurrent nets are used to find patterns in sequence of data such as text, time series data, speech recognition, etc. The advatage of using recurrent net over feed forward net for such tasks is that unlike feed forward nets, recurrent nets possess some sort of memory which takes into account historical events to predict the future.\nFinal models submitted to the competition were GRU model and TextCNN. Average of two models scored AUC of 0.9786 on test data(Private Leaderboard 2708/4551)\nFollowing diagram shows the architechture of GRU and TextCNN model. Pre-trained Word embedding used was Glove-twitter-27B_200d, maxlength of sentence: 150, vocabulary size: 30000\nThe following model architecture was developed with the help of kaggle kernals posted by Kaggle Masters and Keras documentation site.\nGRU\nGRU units: 256, Dense units: 128, dropout: 0.2, learning-rate: 0.65, batch-size: 128, epoch: 4 TextCNN\nFilter-size: (1,2,3,5), Filters: 32, dropout: 0.2, learning-rate: 0.003, batch size=64, epoch=10, patience=3 These models with parameters were finalized after running many iterations with different values for each parameter. I tried 5-Fold cross validation but a single run took more than 5hrs. Due to computation limitations, I decided to skip cross-validation approach and use only hold out set to determine the model performance.\nMetric AUC-ROC (Area under curve-receiver operating curve) was metric used for this problem. ROC curve is a plot of True positive rate vs False positive rate. True positive rate defines number of correct positive results among all positive samples, similarly false positive rate defines number of incorrect positive results among all negative samples. Diagonal represents 50% probability; it is no better than random chance. Points above diagonal represent good classification result that is better than random.\nLong-Short Term Memory Cells Long-Short Term Memory(LSTM) are building units from layers of Recurrent neural networks. It is composed of Cell State, Input Gate, Output Gate and Forget gate. Cell state carries information from one cell to another. Forget gate is made up of sigmoid layer. It outputs the value between 0-1 to determine how much information to store from previous cell state. Input gate determines what new information to store in cell state. It is made up of two layers:\nsigmoid layer: This layer determines which value will update from old cell state tanh layer: This layer creates candidate values to add to cell state The new cell state is calculated as,\nold cell state x forget gate + input gate\nwhere input gate = sigmoid layer x tanh layer. Finally in output layer, sigmoid layer decides which part of cell state is going to output. Then cell state is passed through tanh layer to push the vales between (-1,1) and then multplied with output of sigmoid gate.\nGated Recurrent Units(GRU) Gated Recurrent Units are based on LSTM(Long-Short Term Memory) cells. The difference between LSTM and GRU are as follows:\nInput and Output gates are combined into update gate Cell state and Hidden state are merged together Shiny App I made a application which shows how toxic the sentence is in real time. I couldn\u0026rsquo;t host it since the models were trained on GPU. Following are few screenshots of application.\nConclusion Following are the concepts related to NLP that I learned during this competiton.\nweighted Tfidf TermDocumentMatrix/DocumentTermMatrix WordEmbeddings RNN: GRU and LSTM Implementation of GRU and LSTM with hyperparameter tuning and cross-validation Implementation of CNN for text Classification Metric: AUC-ROC Always trust your cv score rather than public leaderboard score There was lot of confusion regarding overfitting on leaderboard due lots of blends of blends models. Hence I decided to stick with my own models rather than blends of blends and thought this would help me when final results come out. But unfortunately the blends were not overfitted and I went down the leaderboard. It was nice to explore the field which was completely new to me. Moving on to next competetion TalkingData AdTracking Fraud detection Challenge\u0026hellip;\n","permalink":"http://localhost:1313/blog/toxic-comments-classifier/","tags":["NLP"],"title":"Toxic Comment Classifier"}]