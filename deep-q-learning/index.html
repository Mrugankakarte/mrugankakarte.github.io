<!DOCTYPE html>
<html class="no-js" lang="en-us" scroll-behavior="smooth"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="description" content="Reinforcement Learning">
    <meta name="author" content="Mrugank Akarte">

    <!-- Mobile Specific Meta -->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Hugo 0.61.0" />

    <title>Deep Q-Learning | Mrugank Milind Akarte</title>


    <!-- Favicon -->
    <link rel="shortcut icon" type="image/x-icon" href="https://mrugankakarte.github.io/images/favicon.ico"/>

    <!-- CSS -->
    <!-- Fontawesome Icon font -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/themefisher-font/style.css">
    <!-- bootstrap.min css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/bootstrap/dist/css/bootstrap.min.css">
    <!-- Animate.css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/animate-css/animate.css">
    <!-- Magnific popup css -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/magnific-popup/dist/magnific-popup.css">
    <!-- Slick Carousel -->
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick.css">
    <link rel="stylesheet" href="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick-theme.css">
    <!-- Main Stylesheet -->
    
    <link rel="stylesheet" href="https://mrugankakarte.github.io/css/style.min.css" integrity="" media="screen">
    <!-- Custom CSS -->
    
    <link rel="stylesheet" href="https://mrugankakarte.github.io/css/custom.css">
    

</head>
<body id="body" data-spy="scroll" data-target=".navbar" data-offset="52">
        <div id="content">

<!-- Fixed Navigation -->
<nav id="navigation" class="navbar navbar-expand-lg navigation sticky-top">
        <div class="container">

                <!-- logo -->
                <a class="navbar-brand logo" href="https://mrugankakarte.github.io">
                        <img src="https://mrugankakarte.github.io/images/logo.png" alt="Logo" />
                        <span class="pl-2 font-weight-bold">Mrugank Milind Akarte</span>
                </a>
                <!-- /logo -->

                <!-- responsive nav button -->
                <button class="navbar-toggler navbar-dark" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                </button>
                <!-- /responsive nav button -->

                <!-- main nav -->
                
                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav ml-auto navigation-menu">
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#body">Home</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#about">About Me</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#blog">Blog</a></li>
                                
                                <li class="nav-item"><a class="nav-link" data-scroll href="https://mrugankakarte.github.io#contact-us">Contact</a></li>
                                
                        </ul>
                </div>
                
                <!-- /main nav -->
        </div>

</nav>
<!-- End Fixed Navigation -->


<section class="section">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 offset-lg-2 text-center">
                <h1>Deep Q-Learning</h1>
                <ul class="list-inline mb-50">
                    <li class="list-inline-item"><a href="/author/mrugank-akarte">Mrugank Akarte</a></li>
                    <li class="list-inline-item">Saturday, May 18, 2019</li>
                </ul>
                <img class="img-fluid mb-50" src="https://mrugankakarte.github.io/images/blog/deep-q-learning.webp" alt="blog-image">
            </div>
            <div class="col-lg-8 offset-lg-2">
                <div class="post-single-content">
                    <h1 id="deep-q-learning">Deep Q-learning</h1>
<p>We introduce deep neural networks to do the Q-Learning, hence the name Deep Q-Learning. Instead of calculating Q-values for each state-action pair, we calculate Q-values for all actions given the state and then select the action with maximum q-value. This concept was first introduced in <a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> paper. The authors show that they were able to surpass human experts on three out of seven Atari games tested using deep neural networks to solve these reinforcement problems.</p>
<p>The idea in the paper is that, you capture the Atari screen, use convolutional neural networks to extract the features of the game and then calculate the q-values for each action. Then action with maximum q-value is taken and reward, next state are observed. This data (current state, action taken, reward received, next state) are stored in a fixed sized buffer called <strong><em>replay memory</em></strong>. A batch of random samples with uniform distribution are drawn from this memory and then model is updated. The actions taken by our neural network model are considered as predictions and true actions are calculated using the bellman equation. The error between true value and predictions are calcluated as mean squared error and the model is optimized using this error.</p>
<p><img src="/blog/images/ql-dqnalgo.PNG" alt="ql-dqnalgo" title="Deep Q-Learning Algorithm">(<em>Source: Playing Atari with Deep Reinfiorcement Learning, Dec 2013</em>)</p>
<p>Let's try to understand what exactly is happening&hellip;</p>
<p>Similar to Q-Learning, we get the state, execute the action using ε-greedy method and observe reward and next state. We then store these observations into a replay memory and use them to update the weights of our neural network which is used to take action. The neural network maps the state to the actions. In next step we sample the observations randomly from the replay memory and the reason we don't take consecutive observations is that learning is inefficient due to high correlation between the samples. Randomizing the samples break this correlation which helps to reduce the variation of the updates for neural network.</p>
<p>Another reason is that current parameters determine the next data sample that the parameters are trained on. For example, if the maximizing action is to move left then the training samples will be dominated by samples from the left-hand side; if the maximizing action then switches to the right then the training distribution will also switch. This can lead to high divergence in parameters and agent can get stuck in local minima. When randomized samples are used, this behaviour is averaged over many previous states thus smoothing out the learning.</p>
<p>Another confusing thing here is in calculation for values of ground truth. We are using the same network to calculate the <em>y<!-- raw HTML omitted -->i<!-- raw HTML omitted --></em> and also for prediction. It's like chasing your own tail. At everytime step we use our neural net to calculate the ground truth for the sampled states and also predict the Q-values for the same, update the network and repeat. So at each timestep we are changing our parameters which means the groud truth also changes. What this means is that everytime the model tries to reach the ground truth of a state, it changes to a new value. Apparently the model works.</p>
<p>One way to mitigate this problem is by using two neural nets (<em>DQN network</em> and <em>Target network</em>) say with parameters (θ and θ*). Initially we copy the parameters from DQN netwrok to Target network (θ -&gt; θ*), but instead of updating the parameters of target network at each timestep we update them after some fixed time step. By updating the target parameter I mean we copy the new parameter values from DQN network to Targent network after fixed timesteps <em>t</em>. We then use the DQN network for predictions and Target network to calculate the ground truth.</p>
<p>Another solution to above problem is given in <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement learning using Double DQN</a>. The idea is that when we compute the ground truth, we use two networks to decouple the selection from evaluation. So the equation in DQN<br>
<img src="/blog/images/dqn-qtargetdqn.PNG" alt="dqn-qtargetdqn"></p>
<p>changes to</p>
<p><img src="/blog/images/dqn-qtargetddqn.PNG" alt="dqn-qtargetdqn"> (<strong>Notice the θ'</strong>).</p>
<p>This is done because it was found that when same network is used to select the best action and also for evaluation, the DQN tends to over-estimate values. Hence, we use the DQN network to select best action to take for next state (inner Q-value) <em>argmax Q(S<!-- raw HTML omitted -->t+1<!-- raw HTML omitted -->, a; θ<!-- raw HTML omitted -->t<!-- raw HTML omitted -->)</em> and the Target network to calculate <em>Q(S<!-- raw HTML omitted -->t+1<!-- raw HTML omitted -->, argmax Q(S<!-- raw HTML omitted -->t+1<!-- raw HTML omitted -->, a; θ<!-- raw HTML omitted -->t<!-- raw HTML omitted -->); θ&lsquo;<!-- raw HTML omitted -->t<!-- raw HTML omitted -->)</em> the ground truth value of taking that action at next state.</p>
<p>Let's see the DQN in action for <em>Cart and Pole</em> game&hellip;</p>
<p>1.<strong>Agent at the beginning of training&hellip;(Random Actions)</strong></p>

<video width="320" height="240" autoplay loop>
  <source src="/blog/images/dqn-cartpole_ep0.mp4" type="video/mp4">
</video>
<p>2.<strong>Agent after trained for 100 episodes</strong></p>

<video width="320" height="240" autoplay loop>
  <source src="/blog/images/dqn-cartpole_ep100.mp4" type="video/mp4">
</video>
<p>3.<strong>Agent after trained for 200 episodes</strong></p>

<video width="320" height="240" autoplay loop>
  <source src="/blog/images/dqn-cartpole_ep200.mp4" type="video/mp4">
</video>
<p>Isn't it amazing!!! Just a few lines of code and our agent can balance a pole on a cart reasonably well. Infact this is not just limited to <em>Cart and Pole Game</em>, you can even train our agent to play some old school Atari games like SpaceInvaders, PingPong, Breakout and many more. But to get good results you need to train it for a long time. <a href="/files/DQN-Cartpole.html">Here</a> is the link for the complete code for running DQN for Cart and Pole.</p>
<p>In the next article, we will look at <strong><em>Policy Gradient Methods</em></strong> and solve the same <em>Cart and Pole</em> game to compare the results with <em>Deep Q-Learning</em>.</p>

                    
                        

<div class="social-share pt-4">
        <h4>Share:</h4>
        
        <a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_blank" rel="noopener" aria-label="">
            <div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/></svg>
            </div>
        </div>
    </a>

    
    <a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?text=Deep%20Q-Learning&amp;url=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_blank" rel="noopener" aria-label="">
        <div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/></svg>
        </div>
    </div>
</a>


<a class="resp-sharing-button__link" href="https://plus.google.com/share?url=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--google resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M11.37 12.93c-.73-.52-1.4-1.27-1.4-1.5 0-.43.03-.63.98-1.37 1.23-.97 1.9-2.23 1.9-3.57 0-1.22-.36-2.3-1-3.05h.5c.1 0 .2-.04.28-.1l1.36-.98c.16-.12.23-.34.17-.54-.07-.2-.25-.33-.46-.33H7.6c-.66 0-1.34.12-2 .35-2.23.76-3.78 2.66-3.78 4.6 0 2.76 2.13 4.85 5 4.9-.07.23-.1.45-.1.66 0 .43.1.83.33 1.22h-.08c-2.72 0-5.17 1.34-6.1 3.32-.25.52-.37 1.04-.37 1.56 0 .5.13.98.38 1.44.6 1.04 1.84 1.86 3.55 2.28.87.23 1.82.34 2.8.34.88 0 1.7-.1 2.5-.34 2.4-.7 3.97-2.48 3.97-4.54 0-1.97-.63-3.15-2.33-4.35zm-7.7 4.5c0-1.42 1.8-2.68 3.9-2.68h.05c.45 0 .9.07 1.3.2l.42.28c.96.66 1.6 1.1 1.77 1.8.05.16.07.33.07.5 0 1.8-1.33 2.7-3.96 2.7-1.98 0-3.54-1.23-3.54-2.8zM5.54 3.9c.33-.38.75-.58 1.23-.58h.05c1.35.05 2.64 1.55 2.88 3.35.14 1.02-.08 1.97-.6 2.55-.32.37-.74.56-1.23.56h-.03c-1.32-.04-2.63-1.6-2.87-3.4-.13-1 .08-1.92.58-2.5zM23.5 9.5h-3v-3h-2v3h-3v2h3v3h2v-3h3"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="mailto:?subject=Deep%20Q-Learning&amp;body=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_self" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 4H2C.9 4 0 4.9 0 6v12c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17 0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1 0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08 0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f&amp;resubmit=true&amp;title=Deep%20Q-Learning" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M24 11.5c0-1.65-1.35-3-3-3-.96 0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65 0 3-1.35 3-3s-1.35-3-3-3c-1.38 0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65 0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66 0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64 0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4 0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6 0 .23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1 0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1 0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="whatsapp://send?text=Deep%20Q-Learning%20https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--whatsapp resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.1 3.9C17.9 1.7 15 .5 12 .5 5.8.5.7 5.6.7 11.9c0 2 .5 3.9 1.5 5.6L.6 23.4l6-1.6c1.6.9 3.5 1.3 5.4 1.3 6.3 0 11.4-5.1 11.4-11.4-.1-2.8-1.2-5.7-3.3-7.8zM12 21.4c-1.7 0-3.3-.5-4.8-1.3l-.4-.2-3.5 1 1-3.4L4 17c-1-1.5-1.4-3.2-1.4-5.1 0-5.2 4.2-9.4 9.4-9.4 2.5 0 4.9 1 6.7 2.8 1.8 1.8 2.8 4.2 2.8 6.7-.1 5.2-4.3 9.4-9.5 9.4zm5.1-7.1c-.3-.1-1.7-.9-1.9-1-.3-.1-.5-.1-.7.1-.2.3-.8 1-.9 1.1-.2.2-.3.2-.6.1s-1.2-.5-2.3-1.4c-.9-.8-1.4-1.7-1.6-2-.2-.3 0-.5.1-.6s.3-.3.4-.5c.2-.1.3-.3.4-.5.1-.2 0-.4 0-.5C10 9 9.3 7.6 9 7c-.1-.4-.4-.3-.5-.3h-.6s-.4.1-.7.3c-.3.3-1 1-1 2.4s1 2.8 1.1 3c.1.2 2 3.1 4.9 4.3.7.3 1.2.5 1.6.6.7.2 1.3.2 1.8.1.6-.1 1.7-.7 1.9-1.3.2-.7.2-1.2.2-1.3-.1-.3-.3-.4-.6-.5z"/></svg>
    </div>
</div>
</a>


<a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Deep%20Q-Learning&amp;url=https%3a%2f%2fmrugankakarte.github.io%2fdeep-q-learning%2f" target="_blank" rel="noopener" aria-label="">
    <div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--small"><div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M.707 8.475C.275 8.64 0 9.508 0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102 0 0 0 1.75.527l2.96-2.41a.405.405 0 0 1 .494-.013l5.34 3.87a1.1 1.1 0 0 0 1.046.135 1.1 1.1 0 0 0 .682-.803l3.91-18.795A1.102 1.102 0 0 0 22.5.075L.706 8.475z"/></svg>
    </div>
</div>
</a>

</div>
                    
                    

                </div>
            </div>
        </div>
    </div>
</section>


        </div><!-- end Contact Area -->
<footer id="footer" class="bg-one">
	<div class="container">
		<div class="row wow fadeInUp" data-wow-duration="500ms">
			<div class="col-xl-12">

				<!-- Footer Social Links -->
				<div class="social-icon">
					<ul class="list-inline">
						
						<li class="list-inline-item"><a href="https://www.linkedin.com/in/mrugank-akarte/"><i class="tf-ion-social-linkedin"></i></a></li>
						
						<li class="list-inline-item"><a href="https://www.instagram.com/mrugankakarte/"><i class="tf-ion-social-instagram"></i></a></li>
						
						<li class="list-inline-item"><a href="https://github.com/Mrugankakarte"><i class="tf-ion-social-github"></i></a></li>
						
					</ul>
				</div>

				<!-- copyright -->
				<div class="copyright text-center">
					<a href="https://mrugankakarte.github.io">
						<img src="https://mrugankakarte.github.io/images/logo.png" alt="Meghna" />
					</a>
					<br>
					<p>Design And Developed by <a href="http://www.themefisher.com">  Themefisher Team  </a>. Copyright &copy; <script>
							document.write(new Date().getFullYear())
						</script>. All Rights Reserved.</p>
				</div>
			</div>
		</div>
	</div>
</footer>
<!-- /footer -->

<!-- Essential Scripts -->

		<!-- Main jQuery -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/jquery/dist/jquery.min.js"></script>
		<!-- Bootstrap 4.3 + Popper -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/bootstrap/dist/js/bootstrap.bundle.min.js"></script>
		<!-- Slick Carousel -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/slick-carousel/slick/slick.min.js"></script>
		<!-- Portfolio Filtering -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/filterzr/jquery.filterizr.min.js"></script>
		<!-- Magnific popup -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/magnific-popup/dist/jquery.magnific-popup.min.js"></script>
		<!-- Google Map API -->
		<script type="text/javascript"  src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCcABaamniA6OL5YvYSpB3pFMNrXwXnLwU&amp;libraries=places"></script>
		<!-- Number Counter Script -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/count-to/jquery.countTo.js"></script>
		<!-- wow.min Script -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/wow/dist/wow.min.js"></script>
		<!-- Scroll behavior polyfill -->
		
		<script src="https://mrugankakarte.github.io/js/scroll-behavior-polyfill.min.js"></script>
		<!-- Sweet Alert -->
		<script type="text/javascript" src="https://mrugankakarte.github.io/plugins/sweet-alert/sweetalert.min.js"></script>
		<!-- Custom js -->
		
		<script src="https://mrugankakarte.github.io/js/script.min.js"></script>
		<!-- google analitycs -->
		<script>
				(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
				(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
				})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
				ga('create', 'UA-139328350-1', 'auto');
				ga('send', 'pageview');
		</script>

    </body>
</html></body>
</html>
